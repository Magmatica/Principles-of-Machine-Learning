{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/franklin-univ-data-science/comp411/blob/master/Module07_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHK-G76bp0zb"
   },
   "source": [
    " # Homework 8\n",
    "\n",
    "1. Why the holdout method for model selection suggests to separate the data into three parts: a training set, a validation set, and a test set?\n",
    "2. Given a data set (wine), split data (20% test), apply pipeline to standardize the data, classify the data using KNeighborsClassifier (n_neighbors=10), print the test accuracy.\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "df = datasets.load_wine()\n",
    "X = df.data\n",
    "y = df.target\n",
    "```\n",
    "\n",
    "3. What is learning curve? Base on learning curve, how do you know if the model is over fitting or not?\n",
    "4. In the above data set, fit KNN using 10-fold cross validation and grid search to optimize the number of neighbors; print the optimized parameters and the test accuracy.\n",
    "5. Calculate the accuracy, precision and recall based on the following confusion matrix (note that the location of TP, TN, FP, FN are different from those in the textbook).\n",
    "\n",
    "|  | predicted N0 | predicted Yes|\n",
    "|--|--------------| -------------|\n",
    "|Actual No| 50 | 10|\n",
    "|Actual Yes| 5 | 100|\n",
    "\n",
    "6.In two to three paragraphs of prose (i.e. sentences, not bullet lists), summarize and interact with the content that was covered this week in readings and in class meeting. In your summary, you should highlight the major topics, methods, and practices that were covered. Your summary should also interact with the material through personal observations, reflections, and applications to the field of study. In particular, highlight what surprised, enlightened, or otherwise engaged you. In other words, you should think and write critically not just about what was presented but also what you have learned through the session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_3ZBeW3p0ze"
   },
   "source": [
    "1. The holdout method has the data seperated into three parts instead of the known two as the validation set is used to help with the hyperparameters so the model can use the data it is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "df = datasets.load_wine()\n",
    "X = df.data\n",
    "y = df.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=1)\n",
    "\n",
    "pipe_lr = make_pipeline(StandardScaler(), PCA(n_components = 2), KNeighborsClassifier(n_neighbors=10))\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "y_pred = pipe_lr.predict(X_test)\n",
    "print('Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve is a graph that uses the training score and cross-validation score to determine where the model deals with high bias and high variance. The learning curve can be used to determien overfitting as we can see if it has high variance and low bias which would lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9861904761904763\n",
      "{'svc__C': 1.0, 'svc__gamma': 0.1, 'svc__kernel': 'rbf'}\n",
      "Test accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "scores = cross_val_score(estimator=pipe_lr,X=X_train,y=y_train,cv=10,n_jobs=-1)\n",
    "pipe_svc = make_pipeline(StandardScaler(), SVC(random_state=1))\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_grid = [{'svc__C': param_range, 'svc__kernel': ['linear']},{'svc__C': param_range, 'svc__gamma': param_range, 'svc__kernel': ['rbf']}]\n",
    "gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=-1) \n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "clf = gs.best_estimator_ \n",
    "clf.fit(X_train, y_train)\n",
    "print('Test accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = (TP + TN)/(FP + FN + TP + TN) = (100 + 50)/(10 + 5 + 100 + 50) = (150)/(165) = 0.909 \n",
    "Precision = TP/(TP + FP) = 100/(100 + 10) = 100/110 = 0.909\n",
    "Recall = TP/(TP + FN) = 100/(100 + 5) = 100/105 = 0.952"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week we looked at model evaluation and hyperparameter tuning. The pipline uses a transformer to process features and an estimator which is used to fit model and make predictions. Pipelines are used with fit models to predict the outcome based on the given data. The k-fold cross validation is used to deal with hyperparameters as these parameters are set beforehand and are not learned during training. Learning curves are used to check the training accuracy and validation accuracy against the desired accuracy which lets a person determin if the bias is high or low and if the variance is high or low. This lets us check for overfitting and underfitting. The confusion matrix is made with the actual class and the predicted class and uses the shared data to find the accuracy, precision, rates, and recall. The receiver operating characteristic graphs are used to select models for classification based on how they perform with the false positive rate and the true positive rate."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Module07_HW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
