{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/franklin-univ-data-science/comp411/blob/master/Module04_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtTxMnv6oC36"
   },
   "source": [
    "## Homework 5\n",
    "\n",
    "1. Why does Random Forest have a lower variance compared than the decision tree?\n",
    "1. Is KNN a parametric or nonparametric model? Why is that?\n",
    "1. Load the scikit-learn's Wine recognition dataset; separate 20% data as test data set; predict the wine quality using support vector machine's kernel method, decision tree, random forest, k-nearest neighbors, and print the accuracy in the test set of each method. \n",
    "1. In two to three paragraphs of prose (i.e. sentences, not bullet lists), summarize and interact with the content that was covered this week in readings and in class meeting. In your summary, you should highlight the major topics, methods, and practices that were covered. Your summary should also interact with the material through personal observations, reflections, and applications to the field of study. In particular, highlight what surprised, enlightened, or otherwise engaged you. In other words, you should think and write critically not just about what was presented but also what you have learned through the session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest has a lower variance than the decision trees because it is designed to take the information from the decision trees and use them to as their data set making the data more precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is a nonparametric model because it it uses the hyper-parametric K as a starting point to find its neighbors meaning there is no predetermined model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class_0' 'class_1' 'class_2']\n",
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "Labels counts in y: [59 71 48]\n",
      "Labels counts in y_train: [47 57 38]\n",
      "Labels counts in y_test: [12 14 10]\n",
      "Accuracy: 0.97\n",
      "Accuracy: 0.97\n",
      "Accuracy: 0.97\n",
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = datasets.load_wine()\n",
    "X = df.data\n",
    "y = df.target\n",
    "print(df.target_names)\n",
    "print(df.feature_names)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "print('Labels counts in y:', np.bincount(y))\n",
    "print('Labels counts in y_train:', np.bincount(y_train))\n",
    "print('Labels counts in y_test:', np.bincount(y_test))\n",
    "\n",
    "svm = SVC(kernel='rbf', random_state=1, gamma=0.2, C=1.0)\n",
    "svm.fit(X_train_std, y_train)\n",
    "svm = SVC(kernel='rbf', random_state=1, gamma=100.0, C=1.0)\n",
    "svm.fit(X_train_std, y_train)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='gini',  max_depth=4, random_state=1)\n",
    "tree.fit(X_train, y_train)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "forest = RandomForestClassifier(criterion='gini', n_estimators=25, random_state=1, n_jobs=2) \n",
    "forest.fit(X_train, y_train)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "knn.fit(X_train_std, y_train)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week we expanded on the margin classification through the use of kernel methods, trees, forests, and KNN. The kernel methods take data that can not be separated linearly and moves it into a higher-dimensional space with a mapping function letting the dimensional space that contains the data be linearly separated. The decision trees are used to find the most precise grouping of the data which is the information gain. The size of the information gain is based off of the number of impurity for the child nodes. The random forests are the average of multiple decision trees that let it have a lower variance of the overall data and not suffer from overfitting as much. The KNN is used by taking a specific starting point number and uses a determined distance to gather the neighbors to be used in the sample. Overall, these methods are helpful when the data can not be seperated by linear regression."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Module04_HW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
