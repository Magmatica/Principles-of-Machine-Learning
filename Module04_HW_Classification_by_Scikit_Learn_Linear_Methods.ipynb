{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/franklin-univ-data-science/comp411/blob/master/Module04_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtTxMnv6oC36"
   },
   "source": [
    "## Homework 4\n",
    "\n",
    "1. Why do we split samples into training and test set? What does the stratify mean?\n",
    "1. Explain what are the overfitting and regularization.\n",
    "1. Load last week's data set (fruit) and perceptron code, add the feature scaling and run the code again. Why is the convergence much more efficient? \n",
    "1. Load the scikit-learn's Wine recognition dataset; separate 20% data as test data set; predict the wine quality using logistic regression and support vector machine, and print the accuracy using test set. \n",
    "\n",
    "\n",
    "To load the data set:\n",
    "```python\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "df = datasets.load_wine()\n",
    "\n",
    "X = df.data\n",
    "y = df.target\n",
    "\n",
    "```\n",
    "\n",
    "You can check the targets and features by:\n",
    "```python\n",
    "print(df.target_names)\n",
    "print(df.feature_names)\n",
    "```\n",
    "\n",
    "To get more details of the wine data set:https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html\n",
    "\n",
    "5. In two to three paragraphs of prose (i.e. sentences, not bullet lists), summarize and interact with the content that was covered this week in readings and in class meeting. In your summary, you should highlight the major topics, methods, and practices that were covered. Your summary should also interact with the material through personal observations, reflections, and applications to the field of study. In particular, highlight what surprised, enlightened, or otherwise engaged you. In other words, you should think and write critically not just about what was presented but also what you have learned through the session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples are split into training and test sets to see how the model performs using unseen data. The samples start with running given data to help set it up but to make sure it is running properly, it is split to use the unseen data. Stratify means that the samples that have been split will return the training and test subsets that have the same proportions as the input dataset. This is used to compare the unseen data with the input dataset to see how it is running. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is when a model does well when it is given the training data but has trouble with the unseen data. Regulation is a method that is used to prevent overfitting. This method gives additional information that is used to penalize extreme parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/franklin-univ-data-science/data/master/fruit.csv')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "islemon= data.iloc[0:40, 0].values\n",
    "islemon = np.where(islemon == 'lemon', -1, 1)\n",
    "X = data.iloc[0:40, [1, 2]].values\n",
    "class Perceptron():\n",
    "    \n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, islemon):\n",
    "        \n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size= X.shape[1]) \n",
    "        self.errors_ = []\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, islemon):\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w_ += update * xi\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \n",
    "        return X @ self.w_\n",
    "    \n",
    "    def predict(self, X):\n",
    "       \n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)\n",
    "\n",
    "m = islemon.size\n",
    "vector_ones = np.ones(m).reshape(m, 1)\n",
    "X1 = np.hstack([vector_ones, X])\n",
    "\n",
    "ppn = Perceptron(eta=0.1, n_iter=10000)\n",
    "\n",
    "ppn.fit(X1, islemon)\n",
    "\n",
    "plt.plot(range(1, ppn.n_iter + 1), ppn.errors_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of updates')\n",
    "\n",
    "plt.show()\n",
    "islemon_pred = ppn.predict(X1)\n",
    "\n",
    "print('Misclassified samples: %d' % (islemon != islemon_pred).sum())\n",
    "X_train, X_test, islemon_train, islemon_test = train_test_split(\n",
    "    X, islemon, test_size=0.5, random_state=1, stratify=islemon)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convergence is more efficient because the feature scaling helps with creatinga set boundary for the data making it faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class_0' 'class_1' 'class_2']\n",
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "Labels counts in y: [59 71 48]\n",
      "Labels counts in y_train: [47 57 38]\n",
      "Labels counts in y_test: [12 14 10]\n",
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "df = datasets.load_wine()\n",
    "X = df.data\n",
    "y = df.target\n",
    "print(df.target_names)\n",
    "print(df.feature_names)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "print('Labels counts in y:', np.bincount(y))\n",
    "print('Labels counts in y_train:', np.bincount(y_train))\n",
    "print('Labels counts in y_test:', np.bincount(y_test))\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "lr = LogisticRegression(random_state=1, solver='liblinear', multi_class='auto')\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "svm = SVC(kernel='linear', C=1, random_state=1)\n",
    "svm.fit(X_train_std, y_train)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This week we started working with scikit and splitting data. We took the lesson on setting up the data from last week and expanded by using the training and test data sets. These data sets are used to determine if the model can work with unknown data as well as it does known data. This is very useful as there is always the chance that the model would work perfectly with given data but not be able to understand how to work with the unknown. The perceptron that we started last week has been expanded with the accuracy method being added to help with the misclassification error. Then we started on logistic regression which is used to find the probability of the predictions. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Module04_HW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
